{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75954c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GINEConv\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\"\n",
    ")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebe8eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af0bffc1d064f57ac7d613baec8905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphs = {}\n",
    "datasets = set()\n",
    "for p in Path(\"data/tabular/\").glob(\"*.csv\"):\n",
    "    datasets.add(str(p).split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "all_data = {}\n",
    "for dataset in tqdm(datasets):\n",
    "    all_data[dataset] = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    graph_data = pd.read_csv(f\"data/tabular/{dataset}.graph.csv\")\n",
    "    node_features_data = pd.read_csv(\n",
    "        f\"data/tabular/{dataset}.node_features.csv\", index_col=0\n",
    "    )\n",
    "    n_nodes = int(graph_data[\"p2\"].iloc[-1].split(\"_\")[-1]) + 1\n",
    "    n_graphs = int(graph_data.columns[-1]) + 1\n",
    "    train, test = train_test_split(\n",
    "        np.arange(n_graphs),\n",
    "        train_size=0.8,\n",
    "        stratify=node_features_data[\"target\"].to_numpy(),\n",
    "    )\n",
    "    for k in range(n_graphs):\n",
    "        x = []\n",
    "        y = []\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        for r in range(len(graph_data)):\n",
    "            i = int(graph_data[\"p1\"].iloc[r].split(\"_\")[-1])\n",
    "            j = int(graph_data[\"p2\"].iloc[r].split(\"_\")[-1])\n",
    "            v = graph_data[str(k)].iloc[r]\n",
    "            edge_index.append((i, j))\n",
    "            edge_attr.append((v,))\n",
    "        x.append(node_features_data.iloc[k].to_numpy()[:-1])\n",
    "        y = bool(node_features_data.iloc[k].to_numpy()[-1])\n",
    "        data = Data(\n",
    "            x=torch.Tensor(x).T,\n",
    "            edge_index=torch.LongTensor(edge_index).T,\n",
    "            edge_attr=torch.Tensor(edge_attr),\n",
    "            y=y,\n",
    "        )\n",
    "        if k in train:\n",
    "            all_data[dataset][\"train\"].append(data)\n",
    "        else:\n",
    "            all_data[dataset][\"test\"].append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97c6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(all_data, open(\"data/tabular/processed_graphs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure advanced logging\n",
    "def setup_logging(timestamp, dataset_name, model_name, experiment_num):\n",
    "    \"\"\"Setup comprehensive logging system\"\"\"\n",
    "\n",
    "    log_dir = f\"logs/{timestamp}/{dataset_name}/{model_name}/{experiment_num}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    logger = logging.getLogger(f\"{dataset_name}_{model_name}_{experiment_num}\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(f\"{log_dir}/experiment.log\")\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # TensorBoard writer\n",
    "    tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    return logger, tb_writer, log_dir\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    \"\"\"Enhanced GNN model with toggleable components\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        edge_dim,\n",
    "        num_layers=3,\n",
    "        heads=4,\n",
    "        dropout=0.5,\n",
    "        activation=\"relu\",\n",
    "        residual=False,\n",
    "        # ===== TOGGLEABLE COMPONENTS =====\n",
    "        use_classifier_mlp=False,\n",
    "        classifier_mlp_dims=[64, 32],\n",
    "        # =================================\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.activation_name = activation\n",
    "\n",
    "        # === GNN LAYERS ===\n",
    "        _edge_dim = edge_dim\n",
    "        self.per_layer_edge_encoders = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if model_type == \"GINE\":\n",
    "                # Edge encoder for this layer\n",
    "                edge_layer_encoder = nn.Linear(_edge_dim, hidden_channels)\n",
    "                _edge_dim = hidden_channels\n",
    "                self.per_layer_edge_encoders.append(edge_layer_encoder)\n",
    "\n",
    "                # GINE convolution\n",
    "                gin_nn = nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        in_channels if i == 0 else hidden_channels, hidden_channels\n",
    "                    ),\n",
    "                    self._get_activation(activation),\n",
    "                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                )\n",
    "                self.layers.append(GINEConv(gin_nn, edge_dim=hidden_channels))\n",
    "\n",
    "            elif model_type == \"GATv2\":\n",
    "                self.layers.append(\n",
    "                    GATv2Conv(\n",
    "                        in_channels if i == 0 else hidden_channels * heads,\n",
    "                        hidden_channels,\n",
    "                        heads=heads,\n",
    "                        edge_dim=edge_dim,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "        # === CLASSIFIER HEAD ===\n",
    "        classifier_input_dim = (\n",
    "            hidden_channels if model_type == \"GINE\" else hidden_channels * heads\n",
    "        )\n",
    "\n",
    "        if use_classifier_mlp:\n",
    "            classifier_layers = []\n",
    "            in_dim = classifier_input_dim\n",
    "            for dim in classifier_mlp_dims:\n",
    "                classifier_layers.append(nn.Linear(in_dim, dim))\n",
    "                classifier_layers.append(self._get_activation(activation))\n",
    "                in_dim = dim\n",
    "            classifier_layers.append(nn.Linear(dim, out_channels))\n",
    "            self.classifier = nn.Sequential(*classifier_layers)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(classifier_input_dim, out_channels)\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Factory method for activation functions\"\"\"\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(0.1),\n",
    "            \"elu\": nn.ELU(),\n",
    "            \"prelu\": nn.PReLU(),\n",
    "            \"selu\": nn.SELU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "        }\n",
    "        if name.lower() not in activations:\n",
    "            raise ValueError(f\"Unknown activation: {name}\")\n",
    "        return activations[name.lower()]\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            data.batch,\n",
    "        )\n",
    "\n",
    "        x_prev = x\n",
    "        edge_attr_prev = edge_attr\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # GNN layer processing\n",
    "            if self.model_type == \"GINE\":\n",
    "                edge_attr = self.per_layer_edge_encoders[i](edge_attr)\n",
    "                x = layer(x, edge_index, edge_attr)\n",
    "            else:\n",
    "                x = layer(x, edge_index, edge_attr)\n",
    "\n",
    "            # Activation and residual for nodes\n",
    "            x = self.activation(x)\n",
    "            if self.residual and x_prev.shape[1] == x.shape[1]:\n",
    "                x = x + x_prev\n",
    "\n",
    "            # Apply intermediate MLP\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # Update edge attributes\n",
    "            if self.residual and edge_attr_prev.shape[1] == edge_attr.shape[1]:\n",
    "                edge_attr = edge_attr + edge_attr_prev\n",
    "\n",
    "            # Prepare for next layer\n",
    "            x_prev = x\n",
    "            edge_attr_prev = edge_attr\n",
    "\n",
    "        # Global pooling and classification\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class GNNTrainer:\n",
    "    \"\"\"Professional trainer class for GNN experiments\"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.metric_history = defaultdict(list)\n",
    "\n",
    "    def train_epoch(self, model, loader, optimizer, criterion):\n",
    "        \"\"\"Training loop for one epoch\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_probs, all_labels = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            all_probs.append(probs.detach().cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        return total_loss / len(loader.dataset), all_probs, all_labels\n",
    "\n",
    "    def evaluate(self, model, loader, criterion):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        all_probs, all_preds, all_labels = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(self.device)\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y.long())\n",
    "\n",
    "                total_loss += loss.item() * batch.num_graphs\n",
    "                probs = F.softmax(out, dim=1)\n",
    "                preds = probs.argmax(dim=1)\n",
    "\n",
    "                all_probs.append(probs.cpu())\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(batch.y.cpu())\n",
    "\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        metrics = self._compute_metrics(all_probs, all_preds, all_labels)\n",
    "        metrics[\"loss\"] = total_loss / len(loader.dataset)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_metrics(self, probs, preds, labels):\n",
    "        \"\"\"Calculate comprehensive classification metrics\"\"\"\n",
    "        metrics = {}\n",
    "        labels_np = labels.numpy()\n",
    "        probs_np = probs.numpy()\n",
    "        preds_np = preds.numpy()\n",
    "\n",
    "        # Basic metrics\n",
    "        metrics[\"probs\"] = probs_np[:, 1]\n",
    "        metrics[\"accuracy\"] = (preds_np == labels_np).mean()\n",
    "        metrics[\"precision\"] = precision_score(\n",
    "            labels_np, preds_np, average=\"binary\", pos_label=1\n",
    "        )\n",
    "        metrics[\"recall\"] = recall_score(\n",
    "            labels_np, preds_np, average=\"binary\", pos_label=1\n",
    "        )\n",
    "        metrics[\"f1\"] = f1_score(labels_np, preds_np, average=\"binary\", pos_label=1)\n",
    "\n",
    "        # ROC and AUC\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(labels_np, probs_np[:, 1])\n",
    "        except ValueError:\n",
    "            metrics[\"roc_auc\"] = 0.5\n",
    "\n",
    "        # Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(labels_np, probs_np[:, 1])\n",
    "        metrics[\"pr_auc\"] = auc(recall, precision)\n",
    "\n",
    "        # # Confusion matrix\n",
    "        # metrics[\"confusion_matrix\"] = confusion_matrix(labels_np, preds_np)\n",
    "\n",
    "        # Classification report\n",
    "        metrics[\"classification_report\"] = classification_report(\n",
    "            labels_np, preds_np, output_dict=True\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        scheduler,\n",
    "        epochs,\n",
    "        logger,\n",
    "        tb_writer,\n",
    "        log_dir,\n",
    "        patience=128,\n",
    "    ):\n",
    "        \"\"\"Full training loop with early stopping\"\"\"\n",
    "        best_val_f1 = 0\n",
    "        early_stop_counter = 0\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_probs, train_labels = self.train_epoch(\n",
    "                model, train_loader, optimizer, criterion\n",
    "            )\n",
    "            train_metrics = self._compute_metrics(\n",
    "                train_probs, train_probs.argmax(dim=1), train_labels\n",
    "            )\n",
    "\n",
    "            # Validation\n",
    "            val_metrics = self.evaluate(model, val_loader, criterion)\n",
    "\n",
    "            # Update scheduler\n",
    "            scheduler.step(val_metrics[\"f1\"])\n",
    "\n",
    "            # Track history\n",
    "            history[\"epoch\"].append(epoch)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_metrics\"].append(train_metrics)\n",
    "            history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "            history[\"val_roc_auc\"].append(val_metrics[\"roc_auc\"])\n",
    "            history['val_acc'].append(val_metrics['acc'])\n",
    "            history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
    "            history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # Log metrics\n",
    "            # epoch_time = time.time() - start_time\n",
    "            # if epoch % 32 == 0 or epoch == epochs:\n",
    "            #     logger.info(\n",
    "            #         f\"Epoch {epoch:03d} | Time: {epoch_time:.1f}s | \"\n",
    "            #         f\"LR: {history['lr'][-1]:.6f} | \"\n",
    "            #         f\"Train Loss: {train_loss:.4f} | \"\n",
    "            #         f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "            #         f\"Val ROC-AUC: {val_metrics['roc_auc']:.4f} | \"\n",
    "            #         f\"Val F1: {val_metrics['f1']:.4f}\"\n",
    "            #     )\n",
    "\n",
    "            # TensorBoard logging\n",
    "            tb_writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            tb_writer.add_scalar(\"Loss/val\", val_metrics[\"loss\"], epoch)\n",
    "            tb_writer.add_scalar(\"ROC-AUC/val\", val_metrics[\"roc_auc\"], epoch)\n",
    "            tb_writer.add_scalar(\"accuracy/val\", val_metrics[\"acc\"], epoch)\n",
    "            tb_writer.add_scalar(\"F1/val\", val_metrics[\"f1\"], epoch)\n",
    "            tb_writer.add_scalar(\"Learning Rate\", history[\"lr\"][-1], epoch)\n",
    "\n",
    "            # Checkpointing\n",
    "            if val_metrics[\"f1\"] > best_val_f1:\n",
    "                best_val_f1 = val_metrics[\"f1\"]\n",
    "                early_stop_counter = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"val_f1\": best_val_f1,\n",
    "                    },\n",
    "                    f\"{log_dir}/best_model.pth\",\n",
    "                )\n",
    "                # logger.info(f\"New best model: ROC-AUC = {best_val_roc:.4f}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    # logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    early_stop_counter = 0\n",
    "                    break\n",
    "\n",
    "        # Load best model\n",
    "        checkpoint = torch.load(f\"{log_dir}/best_model.pth\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        return history, model\n",
    "\n",
    "\n",
    "def plot_metrics(history, config, log_dir):\n",
    "    \"\"\"Create comprehensive metric visualizations\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Loss curves\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], \"b-\", label=\"Train\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], \"r-\", label=\"Validation\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # # ROC-AUC\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history[\"epoch\"], history[\"val_roc_auc\"], \"g-\")\n",
    "    plt.title(\"Validation ROC-AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ROC-AUC\")\n",
    "\n",
    "    # F1 Score\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history[\"epoch\"], history[\"val_f1\"], \"m-\")\n",
    "    plt.title(\"Validation F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1\")\n",
    "\n",
    "    # # Learning Rate\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history[\"epoch\"], history[\"lr\"], \"c-\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{log_dir}/training_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save history to CSV\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(f\"{log_dir}/training_history.csv\", index=False)\n",
    "\n",
    "\n",
    "def run_experiments(dataset):\n",
    "    \"\"\"Run comprehensive GNN experiments\"\"\"\n",
    "    # Experiment configuration\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_config = {\n",
    "        \"model_types\": [\"GATv2\", \"GINE\"],\n",
    "        \"activations\": [\"leaky_relu\", \"tanh\"],\n",
    "        \"param_grid\": {\n",
    "            # Core GNN parameters\n",
    "            \"learning_rate\": [1e-3, 1e-4],\n",
    "            \"hidden_channels\": [4, 32],\n",
    "            \"dropout\": [0.1, 0.5],\n",
    "            \"num_layers\": [2],\n",
    "            \"heads\": [1, 5],  # For GATv2\n",
    "            \"residual\": [False, True],\n",
    "            # Classifier head configurations\n",
    "            \"use_classifier_mlp\": [False, True],\n",
    "            \"classifier_mlp_dims\": [[32], [16, 16]],\n",
    "        },\n",
    "    }\n",
    "    trainer = GNNTrainer(\n",
    "        device=\"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    for dataset_name in tqdm(dataset):\n",
    "        # Data preparation\n",
    "        train_data = dataset[dataset_name][\"train\"]\n",
    "        test_data = dataset[dataset_name][\"test\"]\n",
    "\n",
    "        for activation in experiment_config[\"activations\"]:\n",
    "            for model_type in experiment_config[\"model_types\"]:\n",
    "                # Hyperparameter tuning with cross-validation\n",
    "                best_score = -1\n",
    "                best_params = {}\n",
    "                current_experument_idx = 0\n",
    "\n",
    "                # Parameter search with cross-validation\n",
    "                for params in tqdm(\n",
    "                    ParameterGrid(experiment_config[\"param_grid\"]),\n",
    "                    desc=f\"experiment: {model_type} with {activation} activation\",\n",
    "                ):\n",
    "                    # Skip invalid configurations\n",
    "                    if (\n",
    "                        model_type == \"GINE\"\n",
    "                        and \"heads\" in params\n",
    "                        and params[\"heads\"] != 1\n",
    "                    ) or (\n",
    "                        params[\"use_classifier_mlp\"] is False\n",
    "                        and params[\"classifier_mlp_dims\"] != [32]\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    cv_scores = []\n",
    "                    skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "                    # Get labels for stratification\n",
    "                    labels = [data.y for data in train_data]\n",
    "\n",
    "                    for fold, (train_idx, val_idx) in enumerate(\n",
    "                        skf.split(train_data, labels)\n",
    "                    ):\n",
    "                        logger, tb_writer, log_dir = setup_logging(\n",
    "                            timestamp,\n",
    "                            dataset,\n",
    "                            f\"{model_type}_{activation}\",\n",
    "                            current_experument_idx,\n",
    "                        )\n",
    "                        current_experument_idx += 1\n",
    "                        # Create data loaders\n",
    "                        train_loader = DataLoader(\n",
    "                            [train_data[i] for i in train_idx],\n",
    "                            batch_size=128,\n",
    "                            shuffle=True,\n",
    "                        )\n",
    "                        val_loader = DataLoader(\n",
    "                            [train_data[i] for i in val_idx], batch_size=128\n",
    "                        )\n",
    "\n",
    "                        # Model initialization\n",
    "                        model = GNNModel(\n",
    "                            model_type=model_type,\n",
    "                            in_channels=1,\n",
    "                            hidden_channels=params[\"hidden_channels\"],\n",
    "                            out_channels=2,\n",
    "                            edge_dim=1,\n",
    "                            num_layers=params[\"num_layers\"],\n",
    "                            heads=params.get(\"heads\", 4),\n",
    "                            dropout=params[\"dropout\"],\n",
    "                            activation=activation,\n",
    "                            residual=params[\"residual\"],\n",
    "                            use_classifier_mlp=params[\"use_classifier_mlp\"],\n",
    "                            classifier_mlp_dims=params[\"classifier_mlp_dims\"],\n",
    "                        ).to(trainer.device)\n",
    "\n",
    "                        # Optimizer and scheduler\n",
    "                        optimizer = torch.optim.AdamW(\n",
    "                            model.parameters(),\n",
    "                            lr=params[\"learning_rate\"],\n",
    "                            weight_decay=1e-5,\n",
    "                        )\n",
    "                        scheduler = ReduceLROnPlateau(\n",
    "                            optimizer,\n",
    "                            mode=\"max\",\n",
    "                            factor=0.5,\n",
    "                            patience=8,\n",
    "                        )\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                        # Training\n",
    "                        try:\n",
    "                            history, model = trainer.train(\n",
    "                                model,\n",
    "                                train_loader,\n",
    "                                val_loader,\n",
    "                                optimizer,\n",
    "                                criterion,\n",
    "                                scheduler,\n",
    "                                epochs=1024,\n",
    "                                logger=logger,\n",
    "                                tb_writer=tb_writer,\n",
    "                                log_dir=log_dir,\n",
    "                                patience=32,\n",
    "                            )\n",
    "\n",
    "                            # Final validation score\n",
    "                            val_metrics = trainer.evaluate(model, val_loader, criterion)\n",
    "                            cv_scores.append(val_metrics[\"roc_auc\"])\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"CV fold {fold} failed: {str(e)}\")\n",
    "                            cv_scores.append(0.0)\n",
    "\n",
    "                    # Average CV score\n",
    "                    mean_score = np.mean(cv_scores)\n",
    "                    # logger.info(f\"Params: {params} | Mean ROC-AUC: {mean_score:.4f}\")\n",
    "\n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_params = params\n",
    "\n",
    "                # logger.info(f\"Best params: {best_params} | ROC-AUC: {best_score:.4f}\")\n",
    "\n",
    "                # Final training with best params on full data\n",
    "                train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "                test_loader = DataLoader(test_data, batch_size=128)\n",
    "\n",
    "                logger, tb_writer, log_dir = setup_logging(\n",
    "                    timestamp,\n",
    "                    dataset,\n",
    "                    f\"{model_type}_{activation}\",\n",
    "                    'final',\n",
    "                )\n",
    "\n",
    "                model = GNNModel(\n",
    "                    model_type=model_type,\n",
    "                    in_channels=1,\n",
    "                    hidden_channels=best_params[\"hidden_channels\"],\n",
    "                    out_channels=2,\n",
    "                    edge_dim=1,\n",
    "                    num_layers=best_params[\"num_layers\"],\n",
    "                    heads=best_params.get(\"heads\", 4),\n",
    "                    dropout=best_params[\"dropout\"],\n",
    "                    activation=activation,\n",
    "                    residual=best_params[\"residual\"],\n",
    "                    use_classifier_mlp=best_params[\"use_classifier_mlp\"],\n",
    "                    classifier_mlp_dims=best_params[\"classifier_mlp_dims\"],\n",
    "                ).to(trainer.device)\n",
    "\n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    model.parameters(),\n",
    "                    lr=best_params[\"learning_rate\"],\n",
    "                    weight_decay=1e-5,\n",
    "                )\n",
    "                scheduler = ReduceLROnPlateau(\n",
    "                    optimizer, mode=\"max\", factor=0.5, patience=8\n",
    "                )\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                # Training with full dataset\n",
    "                history, model = trainer.train(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    test_loader,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    scheduler,\n",
    "                    epochs=1024,\n",
    "                    logger=logger,\n",
    "                    tb_writer=tb_writer,\n",
    "                    log_dir=log_dir,\n",
    "                    patience=32,\n",
    "                )\n",
    "\n",
    "                # Final evaluation\n",
    "                test_metrics = trainer.evaluate(model, test_loader, criterion)\n",
    "\n",
    "                # Log final results\n",
    "                logger.info(f\"\\n{'=' * 50}\")\n",
    "                logger.info(f\"FINAL RESULTS: {model_type} with {activation}\")\n",
    "                logger.info(f\"Test ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "                logger.info(f\"Test F1: {test_metrics['f1']:.4f}\")\n",
    "                logger.info(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "                logger.info(f\"Test PR-AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "                logger.info(\"\\nClassification Report:\")\n",
    "                logger.info(test_metrics[\"classification_report\"])\n",
    "                logger.info(\"=\" * 50)\n",
    "\n",
    "                # Plot metrics\n",
    "                plot_metrics(history, best_params, log_dir)\n",
    "\n",
    "                # Save confusion matrix\n",
    "                # plt.figure(figsize=(8, 6))\n",
    "                # sns.heatmap(test_metrics[\"confusion_matrix\"], annot=True, fmt=\"d\")\n",
    "                # plt.title(\"Confusion Matrix\")\n",
    "                # plt.savefig(f\"{log_dir}/confusion_matrix.png\")\n",
    "                # plt.close()\n",
    "\n",
    "                # Save ROC curve\n",
    "                # fpr, tpr, _ = roc_curve(\n",
    "                #     np.array([x.y for x in test_loader.dataset]),\n",
    "                #     test_metrics[\"probs\"][:, 1].numpy(),\n",
    "                # )\n",
    "                # plt.figure(figsize=(8, 6))\n",
    "                # plt.plot(fpr, tpr, label=f\"ROC (AUC = {test_metrics['roc_auc']:.2f})\")\n",
    "                # plt.plot([0, 1], [0, 1], \"k--\")\n",
    "                # plt.xlabel(\"False Positive Rate\")\n",
    "                # plt.ylabel(\"True Positive Rate\")\n",
    "                # plt.title(\"ROC Curve\")\n",
    "                # plt.legend()\n",
    "                # plt.savefig(f\"{log_dir}/roc_curve.png\")\n",
    "                # plt.close()\n",
    "\n",
    "                # Close TensorBoard writer\n",
    "                tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd78b998184a4285f9eeebc6435fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c637f579884ea0b9000abb2872e0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GATv2 with leaky_relu activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:52:47,066 - Ionoshp_GATv2_leaky_relu_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 10:52:47,067 - Ionoshp_GATv2_leaky_relu_final - INFO - FINAL RESULTS: GATv2 with leaky_relu\n",
      "2025-07-23 10:52:47,067 - Ionoshp_GATv2_leaky_relu_final - INFO - Test ROC-AUC: 0.9939\n",
      "2025-07-23 10:52:47,067 - Ionoshp_GATv2_leaky_relu_final - INFO - Test F1: 0.7931\n",
      "2025-07-23 10:52:47,068 - Ionoshp_GATv2_leaky_relu_final - INFO - Test Accuracy: 0.6620\n",
      "2025-07-23 10:52:47,068 - Ionoshp_GATv2_leaky_relu_final - INFO - Test PR-AUC: 0.9965\n",
      "2025-07-23 10:52:47,068 - Ionoshp_GATv2_leaky_relu_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 10:52:47,068 - Ionoshp_GATv2_leaky_relu_final - INFO - {'False': {'precision': 1.0, 'recall': 0.04, 'f1-score': 0.07692307692307693, 'support': 25.0}, 'True': {'precision': 0.6571428571428571, 'recall': 1.0, 'f1-score': 0.7931034482758621, 'support': 46.0}, 'accuracy': 0.6619718309859155, 'macro avg': {'precision': 0.8285714285714285, 'recall': 0.52, 'f1-score': 0.4350132625994695, 'support': 71.0}, 'weighted avg': {'precision': 0.7778672032193159, 'recall': 0.6619718309859155, 'f1-score': 0.5409272611798109, 'support': 71.0}}\n",
      "2025-07-23 10:52:47,069 - Ionoshp_GATv2_leaky_relu_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951f8e0380b44070b4e5a48b157543c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GINE with leaky_relu activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 11:03:13,754 - Ionoshp_GINE_leaky_relu_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 11:03:13,754 - Ionoshp_GINE_leaky_relu_final - INFO - FINAL RESULTS: GINE with leaky_relu\n",
      "2025-07-23 11:03:13,754 - Ionoshp_GINE_leaky_relu_final - INFO - Test ROC-AUC: 0.9852\n",
      "2025-07-23 11:03:13,754 - Ionoshp_GINE_leaky_relu_final - INFO - Test F1: 0.9783\n",
      "2025-07-23 11:03:13,755 - Ionoshp_GINE_leaky_relu_final - INFO - Test Accuracy: 0.9718\n",
      "2025-07-23 11:03:13,755 - Ionoshp_GINE_leaky_relu_final - INFO - Test PR-AUC: 0.9902\n",
      "2025-07-23 11:03:13,755 - Ionoshp_GINE_leaky_relu_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 11:03:13,755 - Ionoshp_GINE_leaky_relu_final - INFO - {'False': {'precision': 0.96, 'recall': 0.96, 'f1-score': 0.96, 'support': 25.0}, 'True': {'precision': 0.9782608695652174, 'recall': 0.9782608695652174, 'f1-score': 0.9782608695652174, 'support': 46.0}, 'accuracy': 0.971830985915493, 'macro avg': {'precision': 0.9691304347826086, 'recall': 0.9691304347826086, 'f1-score': 0.9691304347826086, 'support': 71.0}, 'weighted avg': {'precision': 0.971830985915493, 'recall': 0.971830985915493, 'f1-score': 0.971830985915493, 'support': 71.0}}\n",
      "2025-07-23 11:03:13,755 - Ionoshp_GINE_leaky_relu_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5052cf4c764b12bfabe5cbc0525205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GATv2 with tanh activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 11:35:01,505 - Ionoshp_GATv2_tanh_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 11:35:01,505 - Ionoshp_GATv2_tanh_final - INFO - FINAL RESULTS: GATv2 with tanh\n",
      "2025-07-23 11:35:01,505 - Ionoshp_GATv2_tanh_final - INFO - Test ROC-AUC: 0.9939\n",
      "2025-07-23 11:35:01,506 - Ionoshp_GATv2_tanh_final - INFO - Test F1: 0.9020\n",
      "2025-07-23 11:35:01,506 - Ionoshp_GATv2_tanh_final - INFO - Test Accuracy: 0.8592\n",
      "2025-07-23 11:35:01,506 - Ionoshp_GATv2_tanh_final - INFO - Test PR-AUC: 0.9965\n",
      "2025-07-23 11:35:01,506 - Ionoshp_GATv2_tanh_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 11:35:01,507 - Ionoshp_GATv2_tanh_final - INFO - {'False': {'precision': 1.0, 'recall': 0.6, 'f1-score': 0.75, 'support': 25.0}, 'True': {'precision': 0.8214285714285714, 'recall': 1.0, 'f1-score': 0.9019607843137255, 'support': 46.0}, 'accuracy': 0.8591549295774648, 'macro avg': {'precision': 0.9107142857142857, 'recall': 0.8, 'f1-score': 0.8259803921568627, 'support': 71.0}, 'weighted avg': {'precision': 0.8843058350100603, 'recall': 0.8591549295774648, 'f1-score': 0.8484534658933996, 'support': 71.0}}\n",
      "2025-07-23 11:35:01,507 - Ionoshp_GATv2_tanh_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7784ec38061e4c739db5432cd3518741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GINE with tanh activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 11:44:16,524 - Ionoshp_GINE_tanh_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 11:44:16,525 - Ionoshp_GINE_tanh_final - INFO - FINAL RESULTS: GINE with tanh\n",
      "2025-07-23 11:44:16,525 - Ionoshp_GINE_tanh_final - INFO - Test ROC-AUC: 0.9870\n",
      "2025-07-23 11:44:16,525 - Ionoshp_GINE_tanh_final - INFO - Test F1: 0.7863\n",
      "2025-07-23 11:44:16,525 - Ionoshp_GINE_tanh_final - INFO - Test Accuracy: 0.6479\n",
      "2025-07-23 11:44:16,526 - Ionoshp_GINE_tanh_final - INFO - Test PR-AUC: 0.9916\n",
      "2025-07-23 11:44:16,526 - Ionoshp_GINE_tanh_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 11:44:16,526 - Ionoshp_GINE_tanh_final - INFO - {'False': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 25.0}, 'True': {'precision': 0.647887323943662, 'recall': 1.0, 'f1-score': 0.7863247863247863, 'support': 46.0}, 'accuracy': 0.647887323943662, 'macro avg': {'precision': 0.323943661971831, 'recall': 0.5, 'f1-score': 0.39316239316239315, 'support': 71.0}, 'weighted avg': {'precision': 0.4197579845268796, 'recall': 0.647887323943662, 'f1-score': 0.5094498615625376, 'support': 71.0}}\n",
      "2025-07-23 11:44:16,526 - Ionoshp_GINE_tanh_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b30945970f46879191d574a35f52c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GATv2 with leaky_relu activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 11:51:34,586 - Cryotherapy_GATv2_leaky_relu_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 11:51:34,587 - Cryotherapy_GATv2_leaky_relu_final - INFO - FINAL RESULTS: GATv2 with leaky_relu\n",
      "2025-07-23 11:51:34,587 - Cryotherapy_GATv2_leaky_relu_final - INFO - Test ROC-AUC: 0.8750\n",
      "2025-07-23 11:51:34,587 - Cryotherapy_GATv2_leaky_relu_final - INFO - Test F1: 0.4615\n",
      "2025-07-23 11:51:34,588 - Cryotherapy_GATv2_leaky_relu_final - INFO - Test Accuracy: 0.6111\n",
      "2025-07-23 11:51:34,588 - Cryotherapy_GATv2_leaky_relu_final - INFO - Test PR-AUC: 0.9292\n",
      "2025-07-23 11:51:34,588 - Cryotherapy_GATv2_leaky_relu_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 11:51:34,589 - Cryotherapy_GATv2_leaky_relu_final - INFO - {'False': {'precision': 0.5333333333333333, 'recall': 1.0, 'f1-score': 0.6956521739130435, 'support': 8.0}, 'True': {'precision': 1.0, 'recall': 0.3, 'f1-score': 0.46153846153846156, 'support': 10.0}, 'accuracy': 0.6111111111111112, 'macro avg': {'precision': 0.7666666666666666, 'recall': 0.65, 'f1-score': 0.5785953177257526, 'support': 18.0}, 'weighted avg': {'precision': 0.7925925925925925, 'recall': 0.6111111111111112, 'f1-score': 0.5655890003716091, 'support': 18.0}}\n",
      "2025-07-23 11:51:34,589 - Cryotherapy_GATv2_leaky_relu_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1034b7c29a1c418680d1490066d024b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GINE with leaky_relu activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 11:54:03,715 - Cryotherapy_GINE_leaky_relu_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 11:54:03,715 - Cryotherapy_GINE_leaky_relu_final - INFO - FINAL RESULTS: GINE with leaky_relu\n",
      "2025-07-23 11:54:03,715 - Cryotherapy_GINE_leaky_relu_final - INFO - Test ROC-AUC: 0.9000\n",
      "2025-07-23 11:54:03,716 - Cryotherapy_GINE_leaky_relu_final - INFO - Test F1: 0.0000\n",
      "2025-07-23 11:54:03,716 - Cryotherapy_GINE_leaky_relu_final - INFO - Test Accuracy: 0.4444\n",
      "2025-07-23 11:54:03,716 - Cryotherapy_GINE_leaky_relu_final - INFO - Test PR-AUC: 0.9064\n",
      "2025-07-23 11:54:03,716 - Cryotherapy_GINE_leaky_relu_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 11:54:03,716 - Cryotherapy_GINE_leaky_relu_final - INFO - {'False': {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846154, 'support': 8.0}, 'True': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, 'accuracy': 0.4444444444444444, 'macro avg': {'precision': 0.2222222222222222, 'recall': 0.5, 'f1-score': 0.3076923076923077, 'support': 18.0}, 'weighted avg': {'precision': 0.19753086419753085, 'recall': 0.4444444444444444, 'f1-score': 0.27350427350427353, 'support': 18.0}}\n",
      "2025-07-23 11:54:03,717 - Cryotherapy_GINE_leaky_relu_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645f2f5a6f874165b2e73fa155c09c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GATv2 with tanh activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:02:32,734 - Cryotherapy_GATv2_tanh_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 12:02:32,734 - Cryotherapy_GATv2_tanh_final - INFO - FINAL RESULTS: GATv2 with tanh\n",
      "2025-07-23 12:02:32,734 - Cryotherapy_GATv2_tanh_final - INFO - Test ROC-AUC: 0.9125\n",
      "2025-07-23 12:02:32,735 - Cryotherapy_GATv2_tanh_final - INFO - Test F1: 0.7143\n",
      "2025-07-23 12:02:32,735 - Cryotherapy_GATv2_tanh_final - INFO - Test Accuracy: 0.5556\n",
      "2025-07-23 12:02:32,735 - Cryotherapy_GATv2_tanh_final - INFO - Test PR-AUC: 0.9247\n",
      "2025-07-23 12:02:32,735 - Cryotherapy_GATv2_tanh_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 12:02:32,736 - Cryotherapy_GATv2_tanh_final - INFO - {'False': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 8.0}, 'True': {'precision': 0.5555555555555556, 'recall': 1.0, 'f1-score': 0.7142857142857143, 'support': 10.0}, 'accuracy': 0.5555555555555556, 'macro avg': {'precision': 0.2777777777777778, 'recall': 0.5, 'f1-score': 0.35714285714285715, 'support': 18.0}, 'weighted avg': {'precision': 0.30864197530864196, 'recall': 0.5555555555555556, 'f1-score': 0.39682539682539686, 'support': 18.0}}\n",
      "2025-07-23 12:02:32,736 - Cryotherapy_GATv2_tanh_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb4f34bc0fd4d29a3402b580318f2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GINE with tanh activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:05:23,626 - Cryotherapy_GINE_tanh_final - INFO - \n",
      "==================================================\n",
      "2025-07-23 12:05:23,626 - Cryotherapy_GINE_tanh_final - INFO - FINAL RESULTS: GINE with tanh\n",
      "2025-07-23 12:05:23,626 - Cryotherapy_GINE_tanh_final - INFO - Test ROC-AUC: 0.9750\n",
      "2025-07-23 12:05:23,627 - Cryotherapy_GINE_tanh_final - INFO - Test F1: 0.0000\n",
      "2025-07-23 12:05:23,627 - Cryotherapy_GINE_tanh_final - INFO - Test Accuracy: 0.4444\n",
      "2025-07-23 12:05:23,627 - Cryotherapy_GINE_tanh_final - INFO - Test PR-AUC: 0.9826\n",
      "2025-07-23 12:05:23,627 - Cryotherapy_GINE_tanh_final - INFO - \n",
      "Classification Report:\n",
      "2025-07-23 12:05:23,628 - Cryotherapy_GINE_tanh_final - INFO - {'False': {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846154, 'support': 8.0}, 'True': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, 'accuracy': 0.4444444444444444, 'macro avg': {'precision': 0.2222222222222222, 'recall': 0.5, 'f1-score': 0.3076923076923077, 'support': 18.0}, 'weighted avg': {'precision': 0.19753086419753085, 'recall': 0.4444444444444444, 'f1-score': 0.27350427350427353, 'support': 18.0}}\n",
      "2025-07-23 12:05:23,628 - Cryotherapy_GINE_tanh_final - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb962a6288a8457789a4ce9a1ba7655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "experiment: GATv2 with leaky_relu activation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_experiments(datasets, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4c84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smiles_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
